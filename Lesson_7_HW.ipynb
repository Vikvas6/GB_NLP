{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запустить seq2seq, seq2seq с вниманием и трансформер для перевода русских слов + описать наблюдения по качеству"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq на уровне токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'rus-eng/rus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 1.1404 - accuracy: 0.7683 - val_loss: 0.9039 - val_accuracy: 0.7577\n",
      "Epoch 2/30\n",
      "125/125 [==============================] - 10s 81ms/step - loss: 0.7308 - accuracy: 0.8040 - val_loss: 0.7764 - val_accuracy: 0.7921\n",
      "Epoch 3/30\n",
      "125/125 [==============================] - 10s 83ms/step - loss: 0.6255 - accuracy: 0.8337 - val_loss: 0.6738 - val_accuracy: 0.8162\n",
      "Epoch 4/30\n",
      "125/125 [==============================] - 10s 84ms/step - loss: 0.5583 - accuracy: 0.8458 - val_loss: 0.6289 - val_accuracy: 0.8231\n",
      "Epoch 5/30\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.5223 - accuracy: 0.8524 - val_loss: 0.6004 - val_accuracy: 0.8301\n",
      "Epoch 6/30\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.5859 - accuracy: 0.8444 - val_loss: 0.6259 - val_accuracy: 0.8237\n",
      "Epoch 7/30\n",
      "125/125 [==============================] - 11s 90ms/step - loss: 0.5086 - accuracy: 0.8560 - val_loss: 0.5769 - val_accuracy: 0.8340\n",
      "Epoch 8/30\n",
      "125/125 [==============================] - 11s 91ms/step - loss: 0.4846 - accuracy: 0.8610 - val_loss: 0.5603 - val_accuracy: 0.8373\n",
      "Epoch 9/30\n",
      "125/125 [==============================] - 12s 93ms/step - loss: 0.4699 - accuracy: 0.8645 - val_loss: 0.5472 - val_accuracy: 0.8414\n",
      "Epoch 10/30\n",
      "125/125 [==============================] - 12s 97ms/step - loss: 0.4582 - accuracy: 0.8671 - val_loss: 0.5389 - val_accuracy: 0.8429\n",
      "Epoch 11/30\n",
      "125/125 [==============================] - 13s 100ms/step - loss: 0.4469 - accuracy: 0.8695 - val_loss: 0.5276 - val_accuracy: 0.8448\n",
      "Epoch 12/30\n",
      "125/125 [==============================] - 13s 103ms/step - loss: 0.4372 - accuracy: 0.8721 - val_loss: 0.5204 - val_accuracy: 0.8475\n",
      "Epoch 13/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.4277 - accuracy: 0.8748 - val_loss: 0.5132 - val_accuracy: 0.8484\n",
      "Epoch 14/30\n",
      "125/125 [==============================] - 12s 99ms/step - loss: 0.4180 - accuracy: 0.8776 - val_loss: 0.5057 - val_accuracy: 0.8514\n",
      "Epoch 15/30\n",
      "125/125 [==============================] - 13s 101ms/step - loss: 0.4092 - accuracy: 0.8801 - val_loss: 0.4985 - val_accuracy: 0.8528\n",
      "Epoch 16/30\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.4009 - accuracy: 0.8829 - val_loss: 0.4909 - val_accuracy: 0.8564\n",
      "Epoch 17/30\n",
      "125/125 [==============================] - 13s 104ms/step - loss: 0.3922 - accuracy: 0.8850 - val_loss: 0.4858 - val_accuracy: 0.8580\n",
      "Epoch 18/30\n",
      "125/125 [==============================] - 13s 105ms/step - loss: 0.3841 - accuracy: 0.8878 - val_loss: 0.4805 - val_accuracy: 0.8610\n",
      "Epoch 19/30\n",
      "125/125 [==============================] - 13s 107ms/step - loss: 0.3755 - accuracy: 0.8900 - val_loss: 0.4759 - val_accuracy: 0.8622\n",
      "Epoch 20/30\n",
      "125/125 [==============================] - 13s 107ms/step - loss: 0.3676 - accuracy: 0.8924 - val_loss: 0.4698 - val_accuracy: 0.8644\n",
      "Epoch 21/30\n",
      "125/125 [==============================] - 13s 107ms/step - loss: 0.3595 - accuracy: 0.8945 - val_loss: 0.4632 - val_accuracy: 0.8661\n",
      "Epoch 22/30\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 0.3516 - accuracy: 0.8973 - val_loss: 0.4584 - val_accuracy: 0.8675\n",
      "Epoch 23/30\n",
      "125/125 [==============================] - 13s 106ms/step - loss: 0.3442 - accuracy: 0.8990 - val_loss: 0.4572 - val_accuracy: 0.8684\n",
      "Epoch 24/30\n",
      "125/125 [==============================] - 14s 108ms/step - loss: 0.3360 - accuracy: 0.9016 - val_loss: 0.4518 - val_accuracy: 0.8706\n",
      "Epoch 25/30\n",
      "125/125 [==============================] - 14s 108ms/step - loss: 0.3291 - accuracy: 0.9039 - val_loss: 0.4492 - val_accuracy: 0.8723\n",
      "Epoch 26/30\n",
      "125/125 [==============================] - 14s 109ms/step - loss: 0.3221 - accuracy: 0.9056 - val_loss: 0.4477 - val_accuracy: 0.8715\n",
      "Epoch 27/30\n",
      "125/125 [==============================] - 14s 111ms/step - loss: 0.3148 - accuracy: 0.9076 - val_loss: 0.4452 - val_accuracy: 0.8737\n",
      "Epoch 28/30\n",
      "125/125 [==============================] - 14s 108ms/step - loss: 0.3072 - accuracy: 0.9100 - val_loss: 0.4409 - val_accuracy: 0.8744\n",
      "Epoch 29/30\n",
      "125/125 [==============================] - 13s 107ms/step - loss: 0.3005 - accuracy: 0.9121 - val_loss: 0.4402 - val_accuracy: 0.8755\n",
      "Epoch 30/30\n",
      "125/125 [==============================] - 14s 110ms/step - loss: 0.2937 - accuracy: 0.9137 - val_loss: 0.4392 - val_accuracy: 0.8761\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Поднишь.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Поднишь.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Поднишь.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Она может.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Она может.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Она может.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Она может.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Она может.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Потери!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Потери!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Потержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Потержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Кто было нав?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Затри!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Затри!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Затри!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Затри!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Затри!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Затри!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Начинь!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Начинь!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгой!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгой!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгой!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгой!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Прекратите!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Прекратите!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Прекратите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подожди.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подожди.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подожди.\n",
      "\n",
      "-\n",
      "Input sentence: Do it.\n",
      "Decoded sentence: Сделай это.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Идите пойти.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Идите пойти.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привет!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привет!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привет!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Привет!\n",
      "\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence: Поставай его.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я подежал.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я подежал.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я подежал.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я подежал.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я проболал.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я проболал.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я проболал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я прободал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я прободал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я прободал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я прободал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я прободал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я прободал.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я прободал.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Она так навая.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Скажи это.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Скажи это.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Скажи это.\n",
      "\n",
      "-\n",
      "Input sentence: Shoot!\n",
      "Decoded sentence: На спаравитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Подними его.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Подними его.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Подними его.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Подними его.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Подними его.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Подними его.\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Прокорите!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ваше зарободе!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ваше зарободе!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ваше зарободе!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ваше зарободе!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Ваше зарободе!\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Содите за ном.\n",
      "\n",
      "-\n",
      "Input sentence: Eat up.\n",
      "Decoded sentence: Подежжай это.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни мержи!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни мержи!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни мержи!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни мержи!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Вставайтесь!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Вставайтесь!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Вставайтесь!\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Подержите это.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Поснолайте!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Слешай это.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Слешай это.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Слешай это.\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Слешай это.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество плохое, зачастую составляются слова, которых нет в русском языке, но надо отметить, что хотя бы похоже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2seq на уровне слов и с механизмом внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "data_path = 'rus-eng/rus.txt'\n",
    "num_samples = 10000\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-ЯёЁ?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.6266\n",
      "Epoch 2 Loss 1.2105\n",
      "Epoch 3 Loss 1.0324\n",
      "Epoch 4 Loss 0.8895\n",
      "Epoch 5 Loss 0.7614\n",
      "Epoch 6 Loss 0.6448\n",
      "Epoch 7 Loss 0.5406\n",
      "Epoch 8 Loss 0.4444\n",
      "Epoch 9 Loss 0.3688\n",
      "Epoch 10 Loss 0.3064\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Input: <start> good morning <end>\n",
      "Predicted translation: как умно ! <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:159: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate('good morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hi <end>\n",
      "Predicted translation: конечно ! <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> me too <end>\n",
      "Predicted translation: отпусти меня ! <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('me too')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> go <end>\n",
      "Predicted translation: не шуми ! <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> good morning for you <end>\n",
      "Predicted translation: будь справедлива ! <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('good morning for you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> happy day to you <end>\n",
      "Predicted translation: дома не чему кто не проиграю ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcgAAAJwCAYAAAAN0PFvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwlVX338c93FpgAokERECUoLuASDYwLrigaCUqixsQo4kLiuCYxxjVqVHyMGyYaeWJANnEhKHGPDwYVFEVjcIkLqEFEVBZBQGHYh9/zR1XD5XJ6GIbuW923P+/Xq1/TVadu3V/N9Nxvn6pTp1JVSJKkG1o2dAGSJC1EBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBuQUSXK3JF9Icp+ha5Gkxc6AnC7PBPYA9h+4Dkla9OJk5dMhSYAzgeOBfYA7VNW6QYuSpEXMHuT0eCRwK+CvgGuAvYctR5IWNwNyejwDOLaqLgOOpjvdKknaSJ5inQJJNgfOAR5XVScluR/wVbrTrBcNW50kLU72IKfDHwMXVNVJAFX1beB/gT8btCpJuglJNk/yjCS3HrqWcQbkdNgP+MDYug/gaVZJC9+fAkfQfY4tKJ5iXeSS3An4CbBLVf3vyPo70o1qvWdV/Wig8iRpvZKcCNweuKyqVg9czg0YkJKkQSTZEfgR8ADga8CuVXXqkDWN8hTrFEiyQ38fZLNt0vVI0gbaDzipHzfxGRbYZSEDcjr8BNh6fGWS2/ZtUyXJiqFrkDQnngG8v//+A8C+s/2yPwQDcjoEaJ0r3wK4YsK1TMI5SQ5MssvQhUjaOEkeDGwHfKRf9WlgM+DRgxU1xmuQi1iSf+6/fSHdKLDLRpqX053Xv6qqHjLp2uZTkucAzwYeCHwdOBQ4pqouHbQwSRssycHAFlW178i6fwVuNbpuSAbkIpbkhP7bR9BNDHDVSPNVdKNYDxwd3TpN+h7k/sDT6XrLHwEOq6qvDFqYpPVKsilwLvDUqjpuZP1Dgc8C2yyEX3gNyEWuP1//YWD/qrpk6HqGkGQ58ALg7cBKukkS3gkcUlXXDlmbpBtLcju6+aI/MP5/NMnTgc9V1bmDFDdaiwG5uPXhcAVw34U0PHoSkmwCPImuF/ko4MvAYcAd6CZtP6mqnE1I0kZxNOAiV1XrkvwU2GToWiYlya50ofhU4GrgKOCFYxMlfB44aZgKJU0De5BTIMkz6cLi6VV1wdD1zLck19A99/JQ4BNVdU1jm82Bg6rq2ZOuT1Jbkp/QHnF/I1V1l3ku5yYZkFMgyXeBO9Ndf/s5sHa0vap+d4i65kuS36mqnw5dh6SbJ8nfjixuAbyEbiT6V/t1u9ONvn9HVR0w4fJuxFOs0+HYoQuYpJlwTPIo4J796tOq6vPDVSXpplTVO2a+T3Ik8Naq+ofRbZK8CrjXhEtrsgepRSfJnYGPAvcBzu5X3wH4LvDHVXXGULVJ2jBJfkM39+rpY+vvCnyzqrYcprLrOZOOFqPDgN8Ad6mqHapqB+AuwMV01yUlLXxrgT0a6/fghpOeDMZTrFOgv93h1XQDdXaguxZ5napaPkRd82h34EFVddbMiqo6K8nfcP21DEkL2z8B/zfJaroneQA8iG7C8tcPVdQoA3I6vBF4CvBmuh+6lwE7An8GvHa4subNWcBvNdavAn424VokbYSqeluSM4G/pntoMsBpwDOr6sODFTbCa5BToB86/fyqOi7JJcD9qurHSZ4P7FlVTx64xDmV5PHA39NNBvDf/er7082e86aq+tRQtUmaHgbkFEhyGbBzf5rxHODxVfWNfjDL/yyEi91zqf8lYFO6CdlnpqlaBqxj7Okl03bs0jRKchvGxsRU1YUDlXMdT7FOh7PoRnGeBZwOPBb4Bt21ussHrGu+vGjoAiTdMkl+B/hX4JHccNzEzOP7Bh87YUBOh48Be9Jd6H4XcHT/SKjt6SbwnipV9b6ha5ikJPfrn7guTZMjgNvQTRt5Nhs4w84keYp1CiV5IPAQ4EdV9emh65kPSVYBT+P6iQJOBY6uqqnrMSe5FvgW3S0sH6qqXw9ckuZYkpesr72q/nFStUxKkkvpRqN/b+haZmNAToEkDwdOHp+TNMkK4MFV9aVhKpsf/WTl/0E3avW7/ep7A1cCj6uqbw5V23xIcje637L3A7aimyThsKo6Yb0v1KLRD7QbtRLYju4SyS8Xwrykc62fIvNZVfWNoWuZjQE5BZKsA7arql+Orb8t3X+uwc/lz6UkpwBnAM+uqrX9us2Bw4Gdqmr1kPXNlyTLgD8Ang3sQzfv7uHA+6rq50PWprmXZBu605DvraqPDV3PXOuninwl8ILx2XQWCgNyCvSn4LapqvPH1t8dOGXaRnImuRzYbfz5l0nuRXe8rXskp0Z/evn5dPe9bgJcQ9er/Nuq+sWQtWluJfk94MNVdbeha5lrY6PRr6T7Ob7OQvjccpDOIpbkk/23BXwgyZUjzcvpTjuePPHC5t8P6Ebtjj8gejvgR5MvZzKSPIDuVOtT6KbaewtdD3I7uskiPk53P6imxzJgm6GLmCcLfjS6Abm4/ar/M8BF3PCWjquALwPvnXRRE/Aa4J+THMANp6h6DfDKJFvNbLgQ7qW6pfoBHPsDd6O79rovcFxVzdwDelaSF9D94qBFKMmTxlfR/eLzQqb0wd+LYTS6p1inQJLXAQfOXI+bdv0p5RkzP8BpLNc0XH9N8r90E7QfUVXnzbLNJsBTF8OHjm5s7Gcaup/j84Ev0J06P2fyVc2//jrrfsBOwGur6oIkDwHOrqrxgUsTZ0BOgX7wBjM9iiTbAo8HTq2qqTvFmuQRG7ptVX1xPmuRtHGS7AZ8HvgJ3fMfd66qM5K8Hrh7VT1tyPrAgJwKSf4f3Sm3dyXZgu5U2+Z0T+z+86o6atACNSeS3IHuaS2bjK6fttt4tDQkOQH4UlW9rh+wc98+IHcH/q2qfmfgEr0GOSV2A17ef/8kugEcd6a7VvVSYCoDcqkERn+cRwMPozv1NjMV14xFfxpZkORxwCvoJr8oukFob62qzwxa2PzZDfjzxvpzWCADk3xg8nS4Fd3DggF+H/hYVV1Nd/1ip8GqmidJ7pDkRLr7AL8CnAicMPI1bd5JNwT+nnQPkn0Y8Cd0jwbaa8C6NEeS/AXdlJE/pgvJV9KdevxYkv2HrG0eXQ78dmP9zsAvG+snzh7kdDgLeEiST9FNVP4n/fqtWCBP5p5j76R7csc96R53tRfdb5wHAH8zYF3z5RF0MwT9IEkB51fVV/rbet4IHD9sefOjv9/zrnS9qR9X1RU38ZLF7BXAS6rqoJF1hyX5Bl1YHj5MWfPqE8Drksx8XlWSHYG3Av8+VFGj7EFOh38E3k/Xo/oFMHOK8eFcPxXbNHkE8Iqq+gH9aL+q+ijdh8wbB61sfvwWcEH//YXA7fvvTwV+d5CK5lGSFUneTnfr0v/Q/QxflORtSVau/9WL1g7AcY31/w8Y/FrcPHkp3S/x5wOb0d2Wdjrwa7pbtgZnD3IKVNXB/fRrOwDHj9wf92PgtcNVNm9agfEjpjQw6AZd7QycCXwbeF6Sn9HdIzeNM+e8DXgq8Dy6D03oTiu/me6X+pcOVNd8Ogt4DF1AjPp94KeTL2f+VdVvgIf2U87tSvdv+82q+tywlV3PgFzkktwa+N2qOonuGZCjLubGs81Mg6UWGO8Ctu2/P4Cup/E0uum5njFUUfPoacD+Y4NTfpzkfLonmkxjQB4IvLufiP9kujMjD6W7R/AvhyxsPox+blXVF+jGS8y0PYTuFrWLBitwphZv81jcktyKbtTXY6vqKyPr7wf8F7B9VV0w2+sXoyT7Aiur6sj+A+U44HZ0gfHMqvrwoAXOsySb0f2CcNa0/dvCdXPt3q+qfji2fmfgW9M6126SJwJ/C+zSrzoNeHtVfWK4qubHYvncMiCnQJIPApdW1XNH1h1Id7PtHw5X2WRMY2Ak2eBBGVU1VaMck3wN+EZVvXBs/XvognP3YSqbP0k+Ttc7/szIJZKpthg+tzzFOh2OAo5O8qKqurqfWedpLILJgDdWkqcAe9Jdf1w2sp6F8p/rFtp6bPnhwLXc8PmXy7h+QNY0eTnwmSSPAb5Kd7pxd7oJ6v9gyMLm0VrgGODXSY4EDl+oj4CaQwv+c8tRrNPheLrbOfbpl/eku3n+U4NVNI/6EY4fAHaku876q7GvRa+q9pn5orsm9VngjlX18Kp6OHAnulPL/zVknfPkTODuwEfoZoPasv/+HnSDWaZOVe3L9U9leTTwoyRfSvKMJFN5SplF8LnlKdYpkeStwD2q6glJjgIuGT9FNS2SnAe8sKqOHbqWSUhyDrDnLM+//HxVbdt+5eK01B4A3tL/2/4F3Ujeq4B/A95ZVacNWtgcW+ifW/Ygp8dRwF5J7gQ8EZjmpzosoxu9ulRsQXd6cdx2dPePTZvxqfRmbAFM82QBwHVTC/4R3QMHrgGOpTtj8J0k0zaCd0F/btmDnCJJ/pvuA+R2VbXLTW2/WCV5E3B1Vb1+6Fomob8mtSfwMm74/Mu3AidU1bOGqWxuJfnn/tsXAkdww1mglgMPAK6qqodMurb51k+A8Ed0z/18DPAtume5Hl1Vl/bb/ClwSFXdZrBC58FC/txykM50eT/dNGyvHrqQuTby4QldD3LffhDHd4CrR7etqr+aZG0T8HzgHcCRwMxMMtfQPSNymnoU9+n/DN2tDleNtF0FfJPufsFpdA7dcX8IeGVVfaexzfF0swtNmwX7uWUPcook2YrupuKDq+rcoeuZS/2jcTZEVdWj5rWYgSTZnG7y+QCnT+sDspMcAfx1P9PKkpBkP+AjUz7fbNNC/twyICVJanCQjiRJDQakJEkNBuQUSrJm6BomaakdLyy9Y/Z4p99CPGYDcjotuB+0ebbUjheW3jF7vNNvwR2zASlJUoOjWOfRJtm0VrH5xN/3aq5kJZtO/H2HMujxZpi3vbquZGUmf8xZsfKmN5oHV117OZssG2hK0uWT70dcte4yNlk+zCRJ12w+zO3x11yxlhWrJv95eeWlF3LNFWub/5OdKGAerWJzHpg9hy5D8ygrltZ/oeXbbjN0CRNXt95i6BIm6oLVWw1dwkSd9ql/mrXNU6ySJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1LMqATLJHkhr/GmnfIcnHklzSf300yR1H2l/fv+ZfxvY7s/6gkXVPT/Lf/X5+meQjSbafzJFKkoayKANyxL2A7YDnzKxIEuDjwDbAo4BHAncAPt63zfgl8JQkm/evWw7sD5w99h6bAK8D7gs8HrgdcPRsBSVZk+SUJKdczZW37OgkSYNZMXQBG2nT/s9fVNWvk1w80vZoujDbqarOBEjyNOB0YE/gc/125wM/BJ4KHArsDZwx/kZVdfjI4hlJng+cluSOVfXzxvaHAIcAbJmtarxdkrQ4LNYe5G2Ba4G1jbZdgLNnwhGgqs6g6xnec2zbg4Hn9t8/t1++gSS7JvlEkp8muQQ4pW/a4RYdgSRpQVusAXkX4GdVdU2jLcBsPbfx9ccDWyV5IrAa+OgNdtSdfv0scBmwH3B/YK++eZONK12StBgs1oB8BHDSLG2nAtsn2XFmRZK70F2HPHV0w6oq4L3A+4D3V9X4RcOd6a45/l1VfamqfgDcfi4OQJK0sC2qa5BJNgH2oRt886dJtu2bbtO3b0t3jfF/gA8m+Su6HuW7gW8CX2js9tD+zw812s4CrgRelOT/0p2+fePcHI0kaSFbbD3IBwPH0tV9LHBO//Xevv2cvlf4BLpBOCcCJwDnAk/o226gqi6oqrdU1VmNtvOBZ/b7O5VuNOtL5viYJEkL0KLqQfa+WFV7tBpm7oXsw+4Js+2gql4PvH6Wtj3Glo8Bjhl/qw0tVpK0OC22HuRVwIXraT9vUoVIkqbboupBVtXJwJPW077tbG2SJN0ci60HKUnSRBiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNawYuoCpFsiKpfNXXOvWDV3CxC3bbLOhS5io0960zdAlTNzOO5w7dAkT9fV7HDN0CRP1gG+eP2ubPUhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGpZ0QCY5MclBY+temuTMkeVnJzk1yRVJfpTkb5Is6b83SVoKVgxdwEKW5DnAAcBfAt8A7g28F7gaOGiW16wB1gCsYrPJFCpJmnP2hNbvtcDLq+rYqvpJVX0KeAvwgtleUFWHVNXqqlq9MptOrFBJ0tyyBwlrkjxrZHklcE6SrYE7AQcnec9I+wogE6xPkjQAAxKOAd4wsvznwFO5vnf9PODkSRclSRqWAQm/rqrTZxaS/Aqgqs5L8gtgp6o6arDqJEmDMCDX7/XAu5NcDHyG7vTrrsD2VfXmIQuTJM0vA3I9qurQJGuBlwFvBi4Hvs8sI1glSdNjSQdkVe3RWHcgcODI8tHA0RMsS5K0AHibhyRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDamqoWuYWltmq3rgskcPXcbkLMWfpWToCiZrCf4bL1u1augSJmrZbbcauoSJOvm8f+PXV53X/I9sD1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpIZFGZBJKsmTR5afnKTGttknyTeSXJHkJ0nelGSTkfYz+/08YGRdkpzRr1/dL5+e5KVj+75bv82u83mckqThLMqAvClJHgt8EDgIuBewP/Bk4B/GNv0F8NyR5d8HrgvRqirgsP71o/YHvl1V35zbyiVJC8ViDcgrgN9aT/urgbdX1RFV9eOqOgF4BfC8JBnZ7gPAE5Pcul9eAxw6tq8jgLsleRBAkuXAM+iC80aSrElySpJTrubKm31gkqSFYbEG5PeAJ4+eMh2zG/DqJJfOfAEfAjYHth3Z7gLgOODpSbYDHg4cM7qjqjoX+DTX9yL3Am5L10O9kao6pKpWV9XqlWy6cUcnSRrciqEL2EgvBj4KXJrkKmD5WPsy4A3ARxqvPX9s+WDg3cBtgKOByxuvORT4UJIX0wXlR6vqoo0vX5K00C3KgKyqryS5A3AnumPYiy7kZnwT2LmqTt+AfX0xyUq6U7C7z7LZccBvgOcB+wB734LyJUmLwKIMSICqWgecCZDk3LHmA4BPJ/kp8GHgGuDewAOq6uWN3b2ALlC/n2TH1nslORx4M93Ans/P0WFIkhaoxXoNcr2q6rPA44BHAl/vv14JnDXL9idU1XtuYreH041wPaIf3SpJmmKLtgc5qqqOBTK27j+B/1zPa3acZf2Z4/vqbQusA47cyDIlSYvIVATkfEqyKd21zv8DfKyqmr1QSdJ0mcpTrHPsqcAP6W7teMnAtUiSJsSAvAlVdWRVLa+qXavqZ0PXI0maDANSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqSGFUMXMPWyhH4HqXVDVyDNuaoauoSJuvaii4cuYbLWzf65tYQ+vSVJ2nAGpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ0GpCRJDQakJEkNBqQkSQ1TH5BJTkxy0MjyvkkuSfKwJLWerz367e+T5HNJLk9yYZIjk9x6sAOSJE3E1AfkqCR/BBwCPBk4Gdiu/3pAv8kDRtadnGQz4Djg0r7ticCDgcMnW7kkadJWDF3ApCR5NPAh4BlV9dl+9bl926p++fyqOnfkNc8EtgD2q6pL+nVrgBOS3LWqTm+8zxpgDcAqNpuvw5EkzbOl0oPcDfg4cBXwlZvxul2A78yEY+9k4Frgnq0XVNUhVbW6qlavZNONrVeSNLClEpAPAl4JfA04+Ga8LkDN0jbbeknSFFgqAXl0VR0E/AXwiCT7beDrTgXum+RWI+seTPf3dtoc1yhJWkCWSkBeCFBVvwD+GnhXkjtswOs+CKwFjupHsz6crgf60db1R0nS9FgqAXmdqnof8GW60aw3te1lwGOBLYGvA58AvgrsP581SpKGN/WjWKtqj8a6PxxbPpPuemPr9d8F9pyP2iRJC9eS60FKkrQhDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhpWDF3ANMvyZSzfYvOhy5iYdZeuHbqEiVuxzdZDlzBR197+t4cuYeJ++Jwthy5hov5w928MXcJEfXvfa2dtswcpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVLDkg7IJCcmOWhs3UuTnDmy/Owkpya5IsmPkvxNkiX99yZJS8GKoQtYyJI8BzgA+EvgG8C9gfcCVwMHreelkqRFzp7Q+r0WeHlVHVtVP6mqTwFvAV4w2wuSrElySpJTrrr2iokVKkmaW/YgYU2SZ40srwTOSbI1cCfg4CTvGWlfAWS2nVXVIcAhALdecbua+3IlSZNgQMIxwBtGlv8ceCrX966fB5w86aIkScMyIOHXVXX6zEKSXwFU1XlJfgHsVFVHDVadJGkQBuT6vR54d5KLgc/QnX7dFdi+qt48ZGGSpPllQK5HVR2aZC3wMuDNwOXA93EEqyRNvSUdkFW1R2PdgcCBI8tHA0dPsCxJ0gLgbR6SJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDWsGLqAaVbrrmXdJZcMXcbkVA1dwcStu+DCoUuYqFy6dugSJu6Ox+8ydAkT9YlV9xu6hIm6+LKTZm2zBylJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUoMBKUlSgwEpSVKDASlJUsNNBmSSE5NU4+vMvv3IJJ9O8pok5yW5NMkRSX5rZB+bJnln335Fkq8leehI+x79Pm83su4D/bon98s7zlLHn43U+a9J3pXkov7r7UmWjezz6Un+O8klSX6Z5CNJth9pn3mP1f3yHfttzkuytj/OnW7R37gkaVHY0B7kEcB2I18HjLU/ArgvsCfwx8DvA28daX8b8BRgf+D3gO8CxyXZrvVmSXYD9pmllr3Gavn4SNu+/THtDjwXWAO8eKR9E+B1fa2PB24HHD1LDZsAxwF3AZ4APATYFPhMkpWz1CZJmhIrNnC7y6rq3JmFJJeMta8Dnl1VlwLfS/IK4LAkr+rbnw/8RVX9R//65wGPAl4IvKbxfgcCbwfe2Gj71WgtY84B/qqqCvhBkrsDLwH+EaCqDh/Z9owkzwdOS3LHqvr52L4eA9wLuFdVndrX/TTgLOBJwDGtApKsoQtmVrHZLGVKkha6uboG+Z0+HGd8la63tlP/tRL4ykxjVa3rt7nn+I6S/BFwV+AdG1HH1/pwHK1j+yRb9vveNcknkvy0D/lT+u12GNvPl4CPAhfPhGNf9/nAD1t1j2xzSFWtrqrVK9l0Iw5BkrQQTGKQTvo/q9E2vm4F3anZV1fV5XNaRLI58FngMmA/4P50p2uhC/NRTwPe0KhPkrREzFVA3qcPoBkPAq4Cfgyc3n8/OihnOd11wlO5oefSBdj7N7KOBybJyPKDgLOr6jfAznTXHP+uqr5UVT8Abj/Lfn4OnAT8dpLreotJtgbuAXx/I+uTJC0ScxWQK4DDk9wryWOAtwDvraq1VbUWeA/wliR7J9mlX94G+Jex/bwMeOnYadKb4w7AO5Pcox/9+jLgn/q2s4ArgRcluUuSx9G+xjnjy8B/Ae9P8qAk9wM+2O/nYxtZnyRpkdjQQTo35Yt0vaoTgM2AfwdePtL+iv7PI4DbAN8C9qqqc8b2c0JVfeEW1PFBYDldsBVwGH1AVtX5SZ4J/APd4KDv0A3gOa61ox7YLIEAAAi9SURBVKqqJH8MvBs4nu5U8YnA3lV19S2oUZK0CGTjO2v9DpIjgdtV1ePnpKKNr+NE4HtV9aIh6xi1ZbaqBy579NBlTM4t/FlajLJy/PL1dMuqpTfwbO2jdhm6hIn62T7XDl3CRJ17wLu58syfp9XmTDqSJDUYkJIkNdzia5BV9aw5qOMWq6o9hq5BkjQ97EFKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1LBi6AKmWVYsZ/lvbzV0GROz7sKLhy5B8+zaSy4ZuoSJ2+LLpw9dwkTtcvLyoUuYqIsvvGrWNnuQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkJIkNRiQkiQ1GJCSJDUYkBsgyYuSfCvJ2iQ/S/KqoWuSJM2vFUMXsEjsCfw98H3g4cChSb5fVZ8ctixJ0nwxIDdAVT1xZPGMJP8A3GmoeiRJ88+AvJmS/B2wEvjoLO1rgDUAq5ZtMcHKJElzyWuQN0OS1wAvBh5TVee0tqmqQ6pqdVWt3mTZqskWKEmaM/YgN1CS2wIHAI+rqm8PXY8kaX7Zg9xwOwIBThu4DknSBBiQG+404P7A2UMXIkmafwbkhrs38AFg66ELkSTNPwNyw20G3INuBKskaco5SGcDVdWJdNcgJUlLgD1ISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaDEhJkhoMSEmSGgxISZIaVgxdwDSra9ax7oJfDV2G5lFdu27oEjTP1l3066FL0DyqddfM2mYPUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkhiUfkElemuTMoeuQJC0sSz4gJUlqWdABmWTLJLeZ8HtunWTVJN9TkrTwLLiATLI8yWOTfAg4F7hvv/7WSQ5J8ssklyT5YpLVI697VpJLk+yZ5HtJ1iY5Icmdx/b/8iTn9tseBWwxVsLewLn9ez1kng9XkrRALZiATHKvJG8DzgKOAdYCewFfShLgP4DtgccDvwd8CfhCku1GdrMp8Cpgf2B34DbAv468x58C/wd4HbAr8EPgJWOlfBB4GnAr4Pgkpyf5+/GglSRNt1TVcG+e3BbYF3gG8LvAccD7gU9W1ZUj2z0K+CSwdVVdPrL+28CHquptSZ4FHAHsXFU/7Nv37detqqprk5wMfL+qnjOyj88Bd62qHRv13Qr4E2A/4GHAV4D3AR+uqktnOaY1wBqAVWy220Oz98b81UhaKJYtH7oCzaP/Wvef/KYuTKtt6B7kXwLvAq4E7lZVf1hVHxkNx95uwGbA+f2p0UuTXArcG9hpZLsrZ8Kxdzawkq4nCbAL8NWxfY8vX6eqLqmqw6vqkcD9gdsDhwFPXs9rDqmq1VW1eiWbzraZJGmBWzHw+x8CXE3Xg/x+ko/R9SA/X1XrRrZbBpxH14sb95uR768Za5vpHm/ULwJJNgUeR9eD3Bv4PvBi4BMbsz9J0uIxaA+yqs6uqjdV1T2ARwOXAv8G/DzJO5L8Xr/pN4FtgGur6vSxr1/ejLc8DXjQ2LobLKfz0CQH0w0SOgg4HditqnatqndV1UU3/2glSYvJ0KdYr1NVX6uq5wPb0Z16vTvw9SQPAz5Hd/3vE0n+IMmdk+ye5A19+4Z6F/DMJM9JcrckrwIeOLbN04H/BLYEngrcqapeVlXfu4WHKElaRIY+xXoj/fXHY4Fjk9weWFdVlWRvuhGo76W7FngeXWgedTP2fUySuwBvorum+UngH4FnjWz2eWDbqvrNjfcgSVoqBh3FOu22zFb1wOw5dBmSbglHsU61hTyKVZKkBcmAlCSpwYCUJKnBgJQkqcGAlCSpwYCUJKnBgJQkqcGAlCSpwYCUJKnBgJQkqcGAlCSpwYCUJKnBgJQkqcGAlCSpwYCUJKnBgJQkqcGAlCSpwYCUJKnBgJQkqcGAlCSpwYCUJKnBgJQkqWHF0AVI0oJ27bqhK9BA7EFKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUYEBKktRgQEqS1GBASpLUsGLoAqZNkjXAGoBVbDZwNZKkjWUPco5V1SFVtbqqVq9k06HLkSRtJANSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqQGA1KSpAYDUpKkBgNSkqSGVNXQNUytJOcDPx3grW8HXDDA+w5lqR0vLL1j9nin31DH/DtVtXWrwYCcQklOqarVQ9cxKUvteGHpHbPHO/0W4jF7ilWSpAYDUpKkBgNyOh0ydAETttSOF5beMXu802/BHbPXICVJarAHKUlSgwEpSVKDASlJUoMBKUlSgwEpSVLD/wc/2zT6NpVPSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('happy day to you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Честно говоря не лучше - теперь возвращаются всегда русские слова, но это потому что токенизировали словами, а сам перевод вообще не соотвествует правде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.4630 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 8.2720 Accuracy 0.0147\n",
      "Epoch 1 Batch 100 Loss 8.0250 Accuracy 0.0517\n",
      "Epoch 1 Loss 7.9328 Accuracy 0.0594\n",
      "Epoch 2 Batch 0 Loss 7.4421 Accuracy 0.0923\n",
      "Epoch 2 Batch 50 Loss 7.2868 Accuracy 0.0955\n",
      "Epoch 2 Batch 100 Loss 7.0779 Accuracy 0.1146\n",
      "Epoch 2 Loss 6.9557 Accuracy 0.1236\n",
      "Epoch 3 Batch 0 Loss 6.3202 Accuracy 0.1634\n",
      "Epoch 3 Batch 50 Loss 5.9757 Accuracy 0.1796\n",
      "Epoch 3 Batch 100 Loss 5.6999 Accuracy 0.1841\n",
      "Epoch 3 Loss 5.5706 Accuracy 0.1848\n",
      "Epoch 4 Batch 0 Loss 4.9440 Accuracy 0.1861\n",
      "Epoch 4 Batch 50 Loss 4.5972 Accuracy 0.1923\n",
      "Epoch 4 Batch 100 Loss 4.4083 Accuracy 0.1978\n",
      "Epoch 4 Loss 4.3259 Accuracy 0.1998\n",
      "Epoch 5 Batch 0 Loss 3.6934 Accuracy 0.2287\n",
      "Epoch 5 Batch 50 Loss 3.6991 Accuracy 0.2146\n",
      "Epoch 5 Batch 100 Loss 3.6169 Accuracy 0.2173\n",
      "Epoch 5 Loss 3.5805 Accuracy 0.2193\n",
      "Epoch 6 Batch 0 Loss 3.1914 Accuracy 0.2443\n",
      "Epoch 6 Batch 50 Loss 3.2522 Accuracy 0.2306\n",
      "Epoch 6 Batch 100 Loss 3.2051 Accuracy 0.2310\n",
      "Epoch 6 Loss 3.1852 Accuracy 0.2313\n",
      "Epoch 7 Batch 0 Loss 3.0091 Accuracy 0.2386\n",
      "Epoch 7 Batch 50 Loss 2.9862 Accuracy 0.2356\n",
      "Epoch 7 Batch 100 Loss 2.9533 Accuracy 0.2365\n",
      "Epoch 7 Loss 2.9393 Accuracy 0.2376\n",
      "Epoch 8 Batch 0 Loss 2.6675 Accuracy 0.2472\n",
      "Epoch 8 Batch 50 Loss 2.7960 Accuracy 0.2406\n",
      "Epoch 8 Batch 100 Loss 2.7609 Accuracy 0.2425\n",
      "Epoch 8 Loss 2.7554 Accuracy 0.2425\n",
      "Epoch 9 Batch 0 Loss 2.4887 Accuracy 0.2457\n",
      "Epoch 9 Batch 50 Loss 2.5539 Accuracy 0.2481\n",
      "Epoch 9 Batch 100 Loss 2.5832 Accuracy 0.2470\n",
      "Epoch 9 Loss 2.5811 Accuracy 0.2474\n",
      "Epoch 10 Batch 0 Loss 2.4681 Accuracy 0.2457\n",
      "Epoch 10 Batch 50 Loss 2.4277 Accuracy 0.2519\n",
      "Epoch 10 Batch 100 Loss 2.4276 Accuracy 0.2521\n",
      "Epoch 10 Loss 2.4278 Accuracy 0.2521\n",
      "Epoch 11 Batch 0 Loss 2.1369 Accuracy 0.2585\n",
      "Epoch 11 Batch 50 Loss 2.2995 Accuracy 0.2556\n",
      "Epoch 11 Batch 100 Loss 2.2941 Accuracy 0.2555\n",
      "Epoch 11 Loss 2.2899 Accuracy 0.2558\n",
      "Epoch 12 Batch 0 Loss 2.0452 Accuracy 0.2685\n",
      "Epoch 12 Batch 50 Loss 2.1744 Accuracy 0.2577\n",
      "Epoch 12 Batch 100 Loss 2.1573 Accuracy 0.2592\n",
      "Epoch 12 Loss 2.1530 Accuracy 0.2594\n",
      "Epoch 13 Batch 0 Loss 2.0387 Accuracy 0.2642\n",
      "Epoch 13 Batch 50 Loss 2.0021 Accuracy 0.2647\n",
      "Epoch 13 Batch 100 Loss 2.0155 Accuracy 0.2644\n",
      "Epoch 13 Loss 2.0166 Accuracy 0.2647\n",
      "Epoch 14 Batch 0 Loss 1.8147 Accuracy 0.2585\n",
      "Epoch 14 Batch 50 Loss 1.8723 Accuracy 0.2684\n",
      "Epoch 14 Batch 100 Loss 1.8832 Accuracy 0.2692\n",
      "Epoch 14 Loss 1.8807 Accuracy 0.2694\n",
      "Epoch 15 Batch 0 Loss 1.6600 Accuracy 0.2812\n",
      "Epoch 15 Batch 50 Loss 1.7564 Accuracy 0.2747\n",
      "Epoch 15 Batch 100 Loss 1.7466 Accuracy 0.2742\n",
      "Epoch 15 Loss 1.7555 Accuracy 0.2745\n",
      "Epoch 16 Batch 0 Loss 1.6190 Accuracy 0.2884\n",
      "Epoch 16 Batch 50 Loss 1.6103 Accuracy 0.2814\n",
      "Epoch 16 Batch 100 Loss 1.6205 Accuracy 0.2804\n",
      "Epoch 16 Loss 1.6245 Accuracy 0.2799\n",
      "Epoch 17 Batch 0 Loss 1.4735 Accuracy 0.2756\n",
      "Epoch 17 Batch 50 Loss 1.4872 Accuracy 0.2858\n",
      "Epoch 17 Batch 100 Loss 1.4952 Accuracy 0.2853\n",
      "Epoch 17 Loss 1.5045 Accuracy 0.2852\n",
      "Epoch 18 Batch 0 Loss 1.3800 Accuracy 0.2912\n",
      "Epoch 18 Batch 50 Loss 1.3689 Accuracy 0.2911\n",
      "Epoch 18 Batch 100 Loss 1.3859 Accuracy 0.2905\n",
      "Epoch 18 Loss 1.3905 Accuracy 0.2902\n",
      "Epoch 19 Batch 0 Loss 1.2291 Accuracy 0.2855\n",
      "Epoch 19 Batch 50 Loss 1.2386 Accuracy 0.2995\n",
      "Epoch 19 Batch 100 Loss 1.2618 Accuracy 0.2967\n",
      "Epoch 19 Loss 1.2736 Accuracy 0.2960\n",
      "Epoch 20 Batch 0 Loss 1.0572 Accuracy 0.3295\n",
      "Epoch 20 Batch 50 Loss 1.1426 Accuracy 0.3044\n",
      "Epoch 20 Batch 100 Loss 1.1572 Accuracy 0.3027\n",
      "Epoch 20 Loss 1.1764 Accuracy 0.3013\n",
      "Epoch 21 Batch 0 Loss 1.0620 Accuracy 0.2884\n",
      "Epoch 21 Batch 50 Loss 1.0461 Accuracy 0.3092\n",
      "Epoch 21 Batch 100 Loss 1.0622 Accuracy 0.3068\n",
      "Epoch 21 Loss 1.0815 Accuracy 0.3054\n",
      "Epoch 22 Batch 0 Loss 0.9110 Accuracy 0.3168\n",
      "Epoch 22 Batch 50 Loss 0.9627 Accuracy 0.3146\n",
      "Epoch 22 Batch 100 Loss 0.9927 Accuracy 0.3107\n",
      "Epoch 22 Loss 1.0040 Accuracy 0.3099\n",
      "Epoch 23 Batch 0 Loss 0.9607 Accuracy 0.3239\n",
      "Epoch 23 Batch 50 Loss 0.8743 Accuracy 0.3202\n",
      "Epoch 23 Batch 100 Loss 0.9197 Accuracy 0.3152\n",
      "Epoch 23 Loss 0.9323 Accuracy 0.3137\n",
      "Epoch 24 Batch 0 Loss 0.8499 Accuracy 0.3196\n",
      "Epoch 24 Batch 50 Loss 0.8003 Accuracy 0.3232\n",
      "Epoch 24 Batch 100 Loss 0.8524 Accuracy 0.3174\n",
      "Epoch 24 Loss 0.8671 Accuracy 0.3165\n",
      "Epoch 25 Batch 0 Loss 0.7326 Accuracy 0.3324\n",
      "Epoch 25 Batch 50 Loss 0.7595 Accuracy 0.3262\n",
      "Epoch 25 Batch 100 Loss 0.8034 Accuracy 0.3207\n",
      "Epoch 25 Loss 0.8221 Accuracy 0.3188\n",
      "Epoch 26 Batch 0 Loss 0.6322 Accuracy 0.3423\n",
      "Epoch 26 Batch 50 Loss 0.6940 Accuracy 0.3308\n",
      "Epoch 26 Batch 100 Loss 0.7456 Accuracy 0.3247\n",
      "Epoch 26 Loss 0.7679 Accuracy 0.3227\n",
      "Epoch 27 Batch 0 Loss 0.5709 Accuracy 0.3523\n",
      "Epoch 27 Batch 50 Loss 0.6713 Accuracy 0.3316\n",
      "Epoch 27 Batch 100 Loss 0.7184 Accuracy 0.3254\n",
      "Epoch 27 Loss 0.7363 Accuracy 0.3240\n",
      "Epoch 28 Batch 0 Loss 0.6274 Accuracy 0.3210\n",
      "Epoch 28 Batch 50 Loss 0.6461 Accuracy 0.3350\n",
      "Epoch 28 Batch 100 Loss 0.6921 Accuracy 0.3291\n",
      "Epoch 28 Loss 0.7119 Accuracy 0.3266\n",
      "Epoch 29 Batch 0 Loss 0.5772 Accuracy 0.3480\n",
      "Epoch 29 Batch 50 Loss 0.6129 Accuracy 0.3373\n",
      "Epoch 29 Batch 100 Loss 0.6622 Accuracy 0.3311\n",
      "Epoch 29 Loss 0.6891 Accuracy 0.3286\n",
      "Epoch 30 Batch 0 Loss 0.6626 Accuracy 0.3267\n",
      "Epoch 30 Batch 50 Loss 0.6131 Accuracy 0.3363\n",
      "Epoch 30 Batch 100 Loss 0.6520 Accuracy 0.3312\n",
      "Epoch 30 Loss 0.6761 Accuracy 0.3290\n",
      "Epoch 31 Batch 0 Loss 0.5581 Accuracy 0.3395\n",
      "Epoch 31 Batch 50 Loss 0.5836 Accuracy 0.3386\n",
      "Epoch 31 Batch 100 Loss 0.6351 Accuracy 0.3324\n",
      "Epoch 31 Loss 0.6594 Accuracy 0.3301\n",
      "Epoch 32 Batch 0 Loss 0.6273 Accuracy 0.3182\n",
      "Epoch 32 Batch 50 Loss 0.5813 Accuracy 0.3368\n",
      "Epoch 32 Batch 100 Loss 0.6390 Accuracy 0.3316\n",
      "Epoch 32 Loss 0.6591 Accuracy 0.3304\n",
      "Epoch 33 Batch 0 Loss 0.5655 Accuracy 0.3523\n",
      "Epoch 33 Batch 50 Loss 0.5606 Accuracy 0.3409\n",
      "Epoch 33 Batch 100 Loss 0.6201 Accuracy 0.3337\n",
      "Epoch 33 Loss 0.6424 Accuracy 0.3313\n",
      "Epoch 34 Batch 0 Loss 0.4959 Accuracy 0.3352\n",
      "Epoch 34 Batch 50 Loss 0.5510 Accuracy 0.3402\n",
      "Epoch 34 Batch 100 Loss 0.6044 Accuracy 0.3357\n",
      "Epoch 34 Loss 0.6246 Accuracy 0.3333\n",
      "Epoch 35 Batch 0 Loss 0.4547 Accuracy 0.3338\n",
      "Epoch 35 Batch 50 Loss 0.5307 Accuracy 0.3422\n",
      "Epoch 35 Batch 100 Loss 0.5873 Accuracy 0.3374\n",
      "Epoch 35 Loss 0.6103 Accuracy 0.3349\n",
      "Epoch 36 Batch 0 Loss 0.4857 Accuracy 0.3523\n",
      "Epoch 36 Batch 50 Loss 0.5030 Accuracy 0.3425\n",
      "Epoch 36 Batch 100 Loss 0.5636 Accuracy 0.3383\n",
      "Epoch 36 Loss 0.5871 Accuracy 0.3365\n",
      "Epoch 37 Batch 0 Loss 0.4245 Accuracy 0.3480\n",
      "Epoch 37 Batch 50 Loss 0.5071 Accuracy 0.3434\n",
      "Epoch 37 Batch 100 Loss 0.5488 Accuracy 0.3399\n",
      "Epoch 37 Loss 0.5732 Accuracy 0.3376\n",
      "Epoch 38 Batch 0 Loss 0.4536 Accuracy 0.3636\n",
      "Epoch 38 Batch 50 Loss 0.4989 Accuracy 0.3444\n",
      "Epoch 38 Batch 100 Loss 0.5380 Accuracy 0.3404\n",
      "Epoch 38 Loss 0.5558 Accuracy 0.3392\n",
      "Epoch 39 Batch 0 Loss 0.4329 Accuracy 0.3480\n",
      "Epoch 39 Batch 50 Loss 0.4715 Accuracy 0.3468\n",
      "Epoch 39 Batch 100 Loss 0.5192 Accuracy 0.3425\n",
      "Epoch 39 Loss 0.5416 Accuracy 0.3397\n",
      "Epoch 40 Batch 0 Loss 0.4243 Accuracy 0.3523\n",
      "Epoch 40 Batch 50 Loss 0.4692 Accuracy 0.3461\n",
      "Epoch 40 Batch 100 Loss 0.5182 Accuracy 0.3418\n",
      "Epoch 40 Loss 0.5367 Accuracy 0.3412\n",
      "Epoch 41 Batch 0 Loss 0.4006 Accuracy 0.3565\n",
      "Epoch 41 Batch 50 Loss 0.4550 Accuracy 0.3496\n",
      "Epoch 41 Batch 100 Loss 0.5004 Accuracy 0.3447\n",
      "Epoch 41 Loss 0.5233 Accuracy 0.3423\n",
      "Epoch 42 Batch 0 Loss 0.4402 Accuracy 0.3551\n",
      "Epoch 42 Batch 50 Loss 0.4543 Accuracy 0.3480\n",
      "Epoch 42 Batch 100 Loss 0.5017 Accuracy 0.3438\n",
      "Epoch 42 Loss 0.5179 Accuracy 0.3421\n",
      "Epoch 43 Batch 0 Loss 0.3760 Accuracy 0.3580\n",
      "Epoch 43 Batch 50 Loss 0.4395 Accuracy 0.3495\n",
      "Epoch 43 Batch 100 Loss 0.4925 Accuracy 0.3448\n",
      "Epoch 43 Loss 0.5089 Accuracy 0.3436\n",
      "Epoch 44 Batch 0 Loss 0.3613 Accuracy 0.3622\n",
      "Epoch 44 Batch 50 Loss 0.4405 Accuracy 0.3508\n",
      "Epoch 44 Batch 100 Loss 0.4764 Accuracy 0.3468\n",
      "Epoch 44 Loss 0.4952 Accuracy 0.3451\n",
      "Epoch 45 Batch 0 Loss 0.4184 Accuracy 0.3622\n",
      "Epoch 45 Batch 50 Loss 0.4277 Accuracy 0.3515\n",
      "Epoch 45 Batch 100 Loss 0.4765 Accuracy 0.3463\n",
      "Epoch 45 Loss 0.4931 Accuracy 0.3444\n",
      "Epoch 46 Batch 0 Loss 0.3490 Accuracy 0.3608\n",
      "Epoch 46 Batch 50 Loss 0.4265 Accuracy 0.3516\n",
      "Epoch 46 Batch 100 Loss 0.4670 Accuracy 0.3474\n",
      "Epoch 46 Loss 0.4900 Accuracy 0.3454\n",
      "Epoch 47 Batch 0 Loss 0.3104 Accuracy 0.3736\n",
      "Epoch 47 Batch 50 Loss 0.4164 Accuracy 0.3546\n",
      "Epoch 47 Batch 100 Loss 0.4619 Accuracy 0.3487\n",
      "Epoch 47 Loss 0.4796 Accuracy 0.3465\n",
      "Epoch 48 Batch 0 Loss 0.2773 Accuracy 0.3622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 50 Loss 0.4282 Accuracy 0.3480\n",
      "Epoch 48 Batch 100 Loss 0.4609 Accuracy 0.3466\n",
      "Epoch 48 Loss 0.4795 Accuracy 0.3456\n",
      "Epoch 49 Batch 0 Loss 0.3836 Accuracy 0.3523\n",
      "Epoch 49 Batch 50 Loss 0.4017 Accuracy 0.3552\n",
      "Epoch 49 Batch 100 Loss 0.4539 Accuracy 0.3487\n",
      "Epoch 49 Loss 0.4683 Accuracy 0.3469\n",
      "Epoch 50 Batch 0 Loss 0.2994 Accuracy 0.3679\n",
      "Epoch 50 Batch 50 Loss 0.3961 Accuracy 0.3546\n",
      "Epoch 50 Batch 100 Loss 0.4447 Accuracy 0.3478\n",
      "Epoch 50 Loss 0.4621 Accuracy 0.3467\n",
      "Epoch 51 Batch 0 Loss 0.3741 Accuracy 0.3381\n",
      "Epoch 51 Batch 50 Loss 0.3958 Accuracy 0.3514\n",
      "Epoch 51 Batch 100 Loss 0.4364 Accuracy 0.3490\n",
      "Epoch 51 Loss 0.4526 Accuracy 0.3478\n",
      "Epoch 52 Batch 0 Loss 0.3544 Accuracy 0.3608\n",
      "Epoch 52 Batch 50 Loss 0.3901 Accuracy 0.3549\n",
      "Epoch 52 Batch 100 Loss 0.4298 Accuracy 0.3512\n",
      "Epoch 52 Loss 0.4481 Accuracy 0.3490\n",
      "Epoch 53 Batch 0 Loss 0.3963 Accuracy 0.3509\n",
      "Epoch 53 Batch 50 Loss 0.3879 Accuracy 0.3542\n",
      "Epoch 53 Batch 100 Loss 0.4319 Accuracy 0.3495\n",
      "Epoch 53 Loss 0.4496 Accuracy 0.3476\n",
      "Epoch 54 Batch 0 Loss 0.2814 Accuracy 0.3636\n",
      "Epoch 54 Batch 50 Loss 0.3872 Accuracy 0.3548\n",
      "Epoch 54 Batch 100 Loss 0.4289 Accuracy 0.3503\n",
      "Epoch 54 Loss 0.4436 Accuracy 0.3490\n",
      "Epoch 55 Batch 0 Loss 0.4132 Accuracy 0.3423\n",
      "Epoch 55 Batch 50 Loss 0.3789 Accuracy 0.3559\n",
      "Epoch 55 Batch 100 Loss 0.4242 Accuracy 0.3504\n",
      "Epoch 55 Loss 0.4405 Accuracy 0.3485\n",
      "Epoch 56 Batch 0 Loss 0.2916 Accuracy 0.3750\n",
      "Epoch 56 Batch 50 Loss 0.3648 Accuracy 0.3555\n",
      "Epoch 56 Batch 100 Loss 0.4103 Accuracy 0.3516\n",
      "Epoch 56 Loss 0.4285 Accuracy 0.3499\n",
      "Epoch 57 Batch 0 Loss 0.3097 Accuracy 0.3622\n",
      "Epoch 57 Batch 50 Loss 0.3654 Accuracy 0.3568\n",
      "Epoch 57 Batch 100 Loss 0.4168 Accuracy 0.3500\n",
      "Epoch 57 Loss 0.4312 Accuracy 0.3493\n",
      "Epoch 58 Batch 0 Loss 0.2758 Accuracy 0.3778\n",
      "Epoch 58 Batch 50 Loss 0.3659 Accuracy 0.3564\n",
      "Epoch 58 Batch 100 Loss 0.4120 Accuracy 0.3512\n",
      "Epoch 58 Loss 0.4266 Accuracy 0.3499\n",
      "Epoch 59 Batch 0 Loss 0.3339 Accuracy 0.3722\n",
      "Epoch 59 Batch 50 Loss 0.3613 Accuracy 0.3560\n",
      "Epoch 59 Batch 100 Loss 0.4026 Accuracy 0.3522\n",
      "Epoch 59 Loss 0.4175 Accuracy 0.3508\n",
      "Epoch 60 Batch 0 Loss 0.3230 Accuracy 0.3651\n",
      "Epoch 60 Batch 50 Loss 0.3653 Accuracy 0.3556\n",
      "Epoch 60 Batch 100 Loss 0.4083 Accuracy 0.3514\n",
      "Epoch 60 Loss 0.4220 Accuracy 0.3498\n",
      "Epoch 61 Batch 0 Loss 0.2814 Accuracy 0.3580\n",
      "Epoch 61 Batch 50 Loss 0.3542 Accuracy 0.3568\n",
      "Epoch 61 Batch 100 Loss 0.4063 Accuracy 0.3518\n",
      "Epoch 61 Loss 0.4222 Accuracy 0.3505\n",
      "Epoch 62 Batch 0 Loss 0.3699 Accuracy 0.3551\n",
      "Epoch 62 Batch 50 Loss 0.3606 Accuracy 0.3556\n",
      "Epoch 62 Batch 100 Loss 0.3961 Accuracy 0.3526\n",
      "Epoch 62 Loss 0.4121 Accuracy 0.3507\n",
      "Epoch 63 Batch 0 Loss 0.3189 Accuracy 0.3622\n",
      "Epoch 63 Batch 50 Loss 0.3572 Accuracy 0.3559\n",
      "Epoch 63 Batch 100 Loss 0.3950 Accuracy 0.3529\n",
      "Epoch 63 Loss 0.4102 Accuracy 0.3512\n",
      "Epoch 64 Batch 0 Loss 0.3542 Accuracy 0.3722\n",
      "Epoch 64 Batch 50 Loss 0.3566 Accuracy 0.3571\n",
      "Epoch 64 Batch 100 Loss 0.3913 Accuracy 0.3536\n",
      "Epoch 64 Loss 0.4088 Accuracy 0.3518\n",
      "Epoch 65 Batch 0 Loss 0.3903 Accuracy 0.3608\n",
      "Epoch 65 Batch 50 Loss 0.3463 Accuracy 0.3591\n",
      "Epoch 65 Batch 100 Loss 0.3910 Accuracy 0.3530\n",
      "Epoch 65 Loss 0.4056 Accuracy 0.3513\n",
      "Epoch 66 Batch 0 Loss 0.2956 Accuracy 0.3693\n",
      "Epoch 66 Batch 50 Loss 0.3438 Accuracy 0.3583\n",
      "Epoch 66 Batch 100 Loss 0.3772 Accuracy 0.3554\n",
      "Epoch 66 Loss 0.3993 Accuracy 0.3527\n",
      "Epoch 67 Batch 0 Loss 0.3438 Accuracy 0.3565\n",
      "Epoch 67 Batch 50 Loss 0.3515 Accuracy 0.3539\n",
      "Epoch 67 Batch 100 Loss 0.3849 Accuracy 0.3517\n",
      "Epoch 67 Loss 0.3969 Accuracy 0.3517\n",
      "Epoch 68 Batch 0 Loss 0.3127 Accuracy 0.3594\n",
      "Epoch 68 Batch 50 Loss 0.3436 Accuracy 0.3569\n",
      "Epoch 68 Batch 100 Loss 0.3814 Accuracy 0.3541\n",
      "Epoch 68 Loss 0.3948 Accuracy 0.3526\n",
      "Epoch 69 Batch 0 Loss 0.3939 Accuracy 0.3452\n",
      "Epoch 69 Batch 50 Loss 0.3501 Accuracy 0.3549\n",
      "Epoch 69 Batch 100 Loss 0.3832 Accuracy 0.3529\n",
      "Epoch 69 Loss 0.3978 Accuracy 0.3524\n",
      "Epoch 70 Batch 0 Loss 0.2955 Accuracy 0.3651\n",
      "Epoch 70 Batch 50 Loss 0.3276 Accuracy 0.3606\n",
      "Epoch 70 Batch 100 Loss 0.3738 Accuracy 0.3546\n",
      "Epoch 70 Loss 0.3926 Accuracy 0.3524\n",
      "Epoch 71 Batch 0 Loss 0.2916 Accuracy 0.3494\n",
      "Epoch 71 Batch 50 Loss 0.3283 Accuracy 0.3599\n",
      "Epoch 71 Batch 100 Loss 0.3722 Accuracy 0.3551\n",
      "Epoch 71 Loss 0.3873 Accuracy 0.3533\n",
      "Epoch 72 Batch 0 Loss 0.2696 Accuracy 0.3565\n",
      "Epoch 72 Batch 50 Loss 0.3329 Accuracy 0.3598\n",
      "Epoch 72 Batch 100 Loss 0.3739 Accuracy 0.3540\n",
      "Epoch 72 Loss 0.3871 Accuracy 0.3525\n",
      "Epoch 73 Batch 0 Loss 0.2886 Accuracy 0.3338\n",
      "Epoch 73 Batch 50 Loss 0.3407 Accuracy 0.3568\n",
      "Epoch 73 Batch 100 Loss 0.3732 Accuracy 0.3536\n",
      "Epoch 73 Loss 0.3845 Accuracy 0.3531\n",
      "Epoch 74 Batch 0 Loss 0.2900 Accuracy 0.3665\n",
      "Epoch 74 Batch 50 Loss 0.3160 Accuracy 0.3612\n",
      "Epoch 74 Batch 100 Loss 0.3595 Accuracy 0.3555\n",
      "Epoch 74 Loss 0.3805 Accuracy 0.3529\n",
      "Epoch 75 Batch 0 Loss 0.2798 Accuracy 0.3565\n",
      "Epoch 75 Batch 50 Loss 0.3322 Accuracy 0.3582\n",
      "Epoch 75 Batch 100 Loss 0.3735 Accuracy 0.3538\n",
      "Epoch 75 Loss 0.3856 Accuracy 0.3527\n",
      "Epoch 76 Batch 0 Loss 0.3358 Accuracy 0.3551\n",
      "Epoch 76 Batch 50 Loss 0.3268 Accuracy 0.3568\n",
      "Epoch 76 Batch 100 Loss 0.3631 Accuracy 0.3549\n",
      "Epoch 76 Loss 0.3776 Accuracy 0.3532\n",
      "Epoch 77 Batch 0 Loss 0.2940 Accuracy 0.3565\n",
      "Epoch 77 Batch 50 Loss 0.3259 Accuracy 0.3594\n",
      "Epoch 77 Batch 100 Loss 0.3577 Accuracy 0.3558\n",
      "Epoch 77 Loss 0.3761 Accuracy 0.3538\n",
      "Epoch 78 Batch 0 Loss 0.2621 Accuracy 0.3636\n",
      "Epoch 78 Batch 50 Loss 0.3274 Accuracy 0.3578\n",
      "Epoch 78 Batch 100 Loss 0.3614 Accuracy 0.3545\n",
      "Epoch 78 Loss 0.3765 Accuracy 0.3534\n",
      "Epoch 79 Batch 0 Loss 0.2379 Accuracy 0.3793\n",
      "Epoch 79 Batch 50 Loss 0.3203 Accuracy 0.3603\n",
      "Epoch 79 Batch 100 Loss 0.3515 Accuracy 0.3559\n",
      "Epoch 79 Loss 0.3677 Accuracy 0.3535\n",
      "Epoch 80 Batch 0 Loss 0.2664 Accuracy 0.3636\n",
      "Epoch 80 Batch 50 Loss 0.3226 Accuracy 0.3572\n",
      "Epoch 80 Batch 100 Loss 0.3544 Accuracy 0.3551\n",
      "Epoch 80 Loss 0.3689 Accuracy 0.3538\n",
      "Epoch 81 Batch 0 Loss 0.2914 Accuracy 0.3679\n",
      "Epoch 81 Batch 50 Loss 0.3127 Accuracy 0.3592\n",
      "Epoch 81 Batch 100 Loss 0.3506 Accuracy 0.3555\n",
      "Epoch 81 Loss 0.3683 Accuracy 0.3538\n",
      "Epoch 82 Batch 0 Loss 0.2877 Accuracy 0.3608\n",
      "Epoch 82 Batch 50 Loss 0.3207 Accuracy 0.3571\n",
      "Epoch 82 Batch 100 Loss 0.3500 Accuracy 0.3549\n",
      "Epoch 82 Loss 0.3654 Accuracy 0.3533\n",
      "Epoch 83 Batch 0 Loss 0.2798 Accuracy 0.3665\n",
      "Epoch 83 Batch 50 Loss 0.3145 Accuracy 0.3595\n",
      "Epoch 83 Batch 100 Loss 0.3518 Accuracy 0.3555\n",
      "Epoch 83 Loss 0.3634 Accuracy 0.3544\n",
      "Epoch 84 Batch 0 Loss 0.2109 Accuracy 0.3764\n",
      "Epoch 84 Batch 50 Loss 0.3069 Accuracy 0.3620\n",
      "Epoch 84 Batch 100 Loss 0.3434 Accuracy 0.3566\n",
      "Epoch 84 Loss 0.3574 Accuracy 0.3549\n",
      "Epoch 85 Batch 0 Loss 0.2974 Accuracy 0.3580\n",
      "Epoch 85 Batch 50 Loss 0.3115 Accuracy 0.3598\n",
      "Epoch 85 Batch 100 Loss 0.3504 Accuracy 0.3553\n",
      "Epoch 85 Loss 0.3655 Accuracy 0.3536\n",
      "Epoch 86 Batch 0 Loss 0.2896 Accuracy 0.3651\n",
      "Epoch 86 Batch 50 Loss 0.3125 Accuracy 0.3594\n",
      "Epoch 86 Batch 100 Loss 0.3431 Accuracy 0.3560\n",
      "Epoch 86 Loss 0.3576 Accuracy 0.3550\n",
      "Epoch 87 Batch 0 Loss 0.2014 Accuracy 0.3608\n",
      "Epoch 87 Batch 50 Loss 0.3156 Accuracy 0.3624\n",
      "Epoch 87 Batch 100 Loss 0.3503 Accuracy 0.3560\n",
      "Epoch 87 Loss 0.3642 Accuracy 0.3544\n",
      "Epoch 88 Batch 0 Loss 0.2260 Accuracy 0.3693\n",
      "Epoch 88 Batch 50 Loss 0.3066 Accuracy 0.3582\n",
      "Epoch 88 Batch 100 Loss 0.3381 Accuracy 0.3561\n",
      "Epoch 88 Loss 0.3525 Accuracy 0.3546\n",
      "Epoch 89 Batch 0 Loss 0.2613 Accuracy 0.3651\n",
      "Epoch 89 Batch 50 Loss 0.3088 Accuracy 0.3606\n",
      "Epoch 89 Batch 100 Loss 0.3423 Accuracy 0.3567\n",
      "Epoch 89 Loss 0.3560 Accuracy 0.3545\n",
      "Epoch 90 Batch 0 Loss 0.3061 Accuracy 0.3622\n",
      "Epoch 90 Batch 50 Loss 0.3065 Accuracy 0.3617\n",
      "Epoch 90 Batch 100 Loss 0.3379 Accuracy 0.3564\n",
      "Epoch 90 Loss 0.3529 Accuracy 0.3552\n",
      "Epoch 91 Batch 0 Loss 0.2737 Accuracy 0.3565\n",
      "Epoch 91 Batch 50 Loss 0.3108 Accuracy 0.3576\n",
      "Epoch 91 Batch 100 Loss 0.3450 Accuracy 0.3548\n",
      "Epoch 91 Loss 0.3551 Accuracy 0.3540\n",
      "Epoch 92 Batch 0 Loss 0.2943 Accuracy 0.3537\n",
      "Epoch 92 Batch 50 Loss 0.3016 Accuracy 0.3612\n",
      "Epoch 92 Batch 100 Loss 0.3352 Accuracy 0.3562\n",
      "Epoch 92 Loss 0.3493 Accuracy 0.3548\n",
      "Epoch 93 Batch 0 Loss 0.2314 Accuracy 0.3807\n",
      "Epoch 93 Batch 50 Loss 0.2941 Accuracy 0.3618\n",
      "Epoch 93 Batch 100 Loss 0.3370 Accuracy 0.3563\n",
      "Epoch 93 Loss 0.3489 Accuracy 0.3555\n",
      "Epoch 94 Batch 0 Loss 0.2618 Accuracy 0.3636\n",
      "Epoch 94 Batch 50 Loss 0.3049 Accuracy 0.3596\n",
      "Epoch 94 Batch 100 Loss 0.3380 Accuracy 0.3556\n",
      "Epoch 94 Loss 0.3504 Accuracy 0.3545\n",
      "Epoch 95 Batch 0 Loss 0.2987 Accuracy 0.3693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 50 Loss 0.3020 Accuracy 0.3604\n",
      "Epoch 95 Batch 100 Loss 0.3388 Accuracy 0.3552\n",
      "Epoch 95 Loss 0.3470 Accuracy 0.3547\n",
      "Epoch 96 Batch 0 Loss 0.2067 Accuracy 0.3807\n",
      "Epoch 96 Batch 50 Loss 0.2988 Accuracy 0.3619\n",
      "Epoch 96 Batch 100 Loss 0.3349 Accuracy 0.3573\n",
      "Epoch 96 Loss 0.3510 Accuracy 0.3548\n",
      "Epoch 97 Batch 0 Loss 0.2067 Accuracy 0.3778\n",
      "Epoch 97 Batch 50 Loss 0.2977 Accuracy 0.3607\n",
      "Epoch 97 Batch 100 Loss 0.3253 Accuracy 0.3576\n",
      "Epoch 97 Loss 0.3417 Accuracy 0.3552\n",
      "Epoch 98 Batch 0 Loss 0.2883 Accuracy 0.3452\n",
      "Epoch 98 Batch 50 Loss 0.2983 Accuracy 0.3607\n",
      "Epoch 98 Batch 100 Loss 0.3293 Accuracy 0.3569\n",
      "Epoch 98 Loss 0.3433 Accuracy 0.3551\n",
      "Epoch 99 Batch 0 Loss 0.2242 Accuracy 0.3821\n",
      "Epoch 99 Batch 50 Loss 0.2985 Accuracy 0.3603\n",
      "Epoch 99 Batch 100 Loss 0.3286 Accuracy 0.3572\n",
      "Epoch 99 Loss 0.3422 Accuracy 0.3555\n",
      "Epoch 100 Batch 0 Loss 0.2896 Accuracy 0.3480\n",
      "Epoch 100 Batch 50 Loss 0.2950 Accuracy 0.3617\n",
      "Epoch 100 Batch 100 Loss 0.3280 Accuracy 0.3568\n",
      "Epoch 100 Loss 0.3417 Accuracy 0.3558\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning.\n",
      "Predicted translation: ['<start>', 'доброе', 'утро', '.']\n"
     ]
    }
   ],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "translate(\"good morning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hi\n",
      "Predicted translation: ['<start>', 'здрасте', '.']\n"
     ]
    }
   ],
   "source": [
    "translate('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: me too\n",
      "Predicted translation: ['<start>', 'меня', 'тоже', '.']\n"
     ]
    }
   ],
   "source": [
    "translate('me too')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: go\n",
      "Predicted translation: ['<start>', 'идите', '.']\n"
     ]
    }
   ],
   "source": [
    "translate('go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning for you\n",
      "Predicted translation: ['<start>', 'рад', 'за', 'вас', '.']\n"
     ]
    }
   ],
   "source": [
    "translate('good morning for you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: happy day to you\n",
      "Predicted translation: ['<start>', 'просто', 'говорил', '.']\n"
     ]
    }
   ],
   "source": [
    "translate('happy day to you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут есть попадания по фразам из датасета, но те примеры, которые я как-то придумал, переведены плохо."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
